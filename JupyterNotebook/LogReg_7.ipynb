{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from joblib import dump\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_data = pd.read_csv('H9/DMR_method_input_NAs.csv')\n",
    "# Load test data (holdout set)\n",
    "test_data = pd.read_csv('H9/DMR_method_holdout_NAs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the features to use\n",
    "features_to_use = ['H3K18ac_1', 'H3K14ac_1', 'H2AFZ_1', 'H3K4me1_1', 'H3K4me2_1', 'H3K4me3_1', 'ATAC_signalValues']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and the target variable\n",
    "X_train = train_data[features_to_use]\n",
    "y_train = train_data['STARR_seq_binary']\n",
    "X_test = test_data[features_to_use]\n",
    "y_test = test_data['STARR_seq_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the specified columns\n",
    "columns_to_drop = ['seqnames', 'start', 'end', 'width', 'strand']\n",
    "train_data = train_data.drop(columns=columns_to_drop)\n",
    "test_data = test_data.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with zero in the datasets\n",
    "X_train_filled = X_train.fillna(0)\n",
    "X_test_filled = X_test.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_filled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'C': uniform(0.01, 10),  # Adjusted range for regularization strength\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],  # Penalty types\n",
    "    'solver': ['saga'],  # Solver to handle L1 and elasticnet\n",
    "    'l1_ratio': uniform(0, 1)  # For elasticnet\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Logistic Regression model\n",
    "logreg_model = LogisticRegression(random_state=42, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=logreg_model, param_distributions=param_dist, n_iter=100, cv=3, n_jobs=-1, verbose=10, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train_balanced, y_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model from RandomizedSearchCV\n",
    "best_logreg_model = random_search.best_estimator_\n",
    "print(f\"Best parameters found: {random_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "dump(best_logreg_model, 'logistic_regression_model_balanced_smote_7.joblib')\n",
    "print(\"Best Logistic Regression model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_roc_curve_data_with_predictions(model, X, y, original_data, filename):\n",
    "    \"\"\"\n",
    "    Computes and saves the ROC curve data and predictions for a given model and dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained model to be evaluated.\n",
    "    - X: Feature matrix.\n",
    "    - y: True labels.\n",
    "    - original_data: Original dataframe containing 'chr', 'start', and 'end' columns.\n",
    "    - filename: Base filename for saving the outputs.\n",
    "\n",
    "    Outputs:\n",
    "    - Saves ROC curve data to a CSV file.\n",
    "    - Saves prediction data to a CSV file.\n",
    "    \"\"\"\n",
    "    # Predict probabilities for the positive class\n",
    "    y_prob = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    # Compute False Positive Rate (FPR), True Positive Rate (TPR), and thresholds\n",
    "    fpr, tpr, thresholds = roc_curve(y, y_prob)\n",
    "\n",
    "    # Compute the Area Under the ROC Curve (AUC)\n",
    "    roc_auc = roc_auc_score(y, y_prob)\n",
    "\n",
    "    # Create a DataFrame to save the ROC curve data\n",
    "    roc_data = pd.DataFrame({\n",
    "        'False Positive Rate': fpr,\n",
    "        'True Positive Rate': tpr,\n",
    "        'Thresholds': thresholds\n",
    "    })\n",
    "\n",
    "    # Create a DataFrame to save the predictions\n",
    "    pred_data = original_data[['chr', 'start', 'end']].copy()\n",
    "    pred_data['Actual'] = y\n",
    "    pred_data['Probabilities'] = y_prob\n",
    "\n",
    "    # Ensure pred_data only contains the specified columns\n",
    "    pred_data = pred_data[['chr', 'start', 'end', 'Actual', 'Probabilities']]\n",
    "\n",
    "    # Save the ROC curve data\n",
    "    roc_data.to_csv(filename.replace('.csv', '_roc.csv'), index=False)\n",
    "    print(f\"ROC AUC: {roc_auc:.2f} saved to {filename.replace('.csv', '_roc.csv')}\")\n",
    "\n",
    "    # Save the prediction data\n",
    "    pred_data.to_csv(filename.replace('.csv', '_predictions.csv'), index=False)\n",
    "    print(f\"Predictions saved to {filename.replace('.csv', '_predictions.csv')}\")\n",
    "\n",
    "    return thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and save ROC curve data for the best Logistic Regression model on test data\n",
    "save_roc_curve_data_with_predictions(best_logreg_model, X_test_filled, y_test, 'roc_curve_log_reg_test_7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve for the test data\n",
    "y_prob_test = best_logreg_model.predict_proba(X_test_filled)[:, 1]\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_prob_test)\n",
    "roc_auc_test = roc_auc_score(y_test, y_prob_test)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr_test, tpr_test, color='darkorange', lw=2, label=f'Test ROC curve (area = {roc_auc_test:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal threshold using Youden's J statistic\n",
    "youden_index = tpr_test - fpr_test\n",
    "optimal_idx = np.argmax(youden_index)\n",
    "optimal_threshold = thresholds_test[optimal_idx]\n",
    "print(f'Optimal Threshold: {optimal_threshold:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the optimal threshold to make new class predictions\n",
    "y_pred_optimal = (y_prob_test >= optimal_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the holdout (test) data with the optimal threshold\n",
    "accuracy_optimal = accuracy_score(y_test, y_pred_optimal)\n",
    "print(f'Accuracy of the Logistic Regression model on holdout data with optimal threshold: {accuracy_optimal:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and print the classification report on the holdout (test) data with the optimal threshold\n",
    "report_optimal = classification_report(y_test, y_pred_optimal, target_names=['0', '1'], digits=2)\n",
    "print(\"Classification Report on holdout data with optimal threshold:\")\n",
    "print(report_optimal)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
