{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from joblib import dump\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_data = pd.read_csv('H9/DMR_method_input_NAs.csv')\n",
    "# Load test data (holdout set)\n",
    "test_data = pd.read_csv('H9/DMR_method_holdout_NAs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and the target variable\n",
    "X_train = train_data.drop('STARR_seq_binary', axis=1)\n",
    "y_train = train_data['STARR_seq_binary']\n",
    "X_test = test_data.drop('STARR_seq_binary', axis=1)\n",
    "y_test = test_data['STARR_seq_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the specified columns\n",
    "columns_to_drop = ['seqnames', 'start', 'end', 'width', 'strand']\n",
    "X_train_dropped = X_train.drop(columns=columns_to_drop)\n",
    "X_test_dropped = X_test.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the features to use\n",
    "features_to_use = ['H3K18ac_1', 'H3K14ac_1', 'H2AFZ_1', 'H3K4me1_1', 'H3K4me2_1', 'H3K4me3_1', 'ATAC_signalValues']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the specified features\n",
    "X_train_selected = X_train_dropped[features_to_use]\n",
    "X_test_selected = X_test_dropped[features_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with zero in the datasets\n",
    "X_train_filled = X_train_selected.fillna(0)\n",
    "X_test_filled = X_test_selected.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_filled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train_balanced, y_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model from GridSearchCV\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "dump(best_rf_model, 'random_forest_model_balanced_smote_best_7.joblib')\n",
    "print(\"Best Random Forest model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_roc_curve_data_with_predictions(model, X, y, original_data, filename):\n",
    "    \"\"\"\n",
    "    Computes and saves the ROC curve data and predictions for a given model and dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained model to be evaluated.\n",
    "    - X: Feature matrix.\n",
    "    - y: True labels.\n",
    "    - original_data: Original dataframe containing 'chr', 'start', and 'end' columns.\n",
    "    - filename: Base filename for saving the outputs.\n",
    "\n",
    "    Outputs:\n",
    "    - Saves ROC curve data to a CSV file.\n",
    "    - Saves prediction data to a CSV file.\n",
    "    \"\"\"\n",
    "    # Predict probabilities for the positive class\n",
    "    y_prob = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    # Compute False Positive Rate (FPR), True Positive Rate (TPR), and thresholds\n",
    "    fpr, tpr, thresholds = roc_curve(y, y_prob)\n",
    "\n",
    "    # Compute the Area Under the ROC Curve (AUC)\n",
    "    roc_auc = roc_auc_score(y, y_prob)\n",
    "\n",
    "    # Create a DataFrame to save the ROC curve data\n",
    "    roc_data = pd.DataFrame({\n",
    "        'False Positive Rate': fpr,\n",
    "        'True Positive Rate': tpr,\n",
    "        'Thresholds': thresholds\n",
    "    })\n",
    "\n",
    "    # Create a DataFrame to save the predictions\n",
    "    pred_data = original_data[['chr', 'start', 'end']].copy()\n",
    "    pred_data['Actual'] = y\n",
    "    pred_data['Probabilities'] = y_prob\n",
    "\n",
    "    # Save the ROC curve data\n",
    "    roc_data.to_csv(filename.replace('.csv', '_roc.csv'), index=False)\n",
    "    print(f\"ROC AUC: {roc_auc:.2f} saved to {filename.replace('.csv', '_roc.csv')}\")\n",
    "\n",
    "    # Save the prediction data\n",
    "    pred_data.to_csv(filename.replace('.csv', '_predictions.csv'), index=False)\n",
    "    print(f\"Predictions saved to {filename.replace('.csv', '_predictions.csv')}\")\n",
    "\n",
    "    return thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and save ROC curve data for the best Random Forest model on test data\n",
    "thresholds = save_roc_curve_data_with_predictions(best_rf_model, X_test_filled, y_test, 'roc_curve_random_forest_test_7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve for the test data\n",
    "y_prob_test = best_rf_model.predict_proba(X_test_filled)[:, 1]\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_prob_test)\n",
    "roc_auc_test = roc_auc_score(y_test, y_prob_test)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr_test, tpr_test, color='darkorange', lw=2, label=f'Test ROC curve (area = {roc_auc_test:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal threshold that maximizes Youdenâ€™s J statistic (TPR - FPR)\n",
    "youden_index = tpr_test - fpr_test\n",
    "optimal_idx = np.argmax(youden_index)\n",
    "optimal_threshold = thresholds_test[optimal_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the optimal threshold to make new class predictions\n",
    "y_pred_optimal = (y_prob_test >= optimal_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the holdout (test) data with the optimal threshold\n",
    "accuracy_optimal = accuracy_score(y_test, y_pred_optimal)\n",
    "print(f'Accuracy of the Random Forest model on holdout data with optimal threshold: {accuracy_optimal}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and print the classification report on the holdout (test) data with the optimal threshold\n",
    "report_optimal = classification_report(y_test, y_pred_optimal, target_names=['0', '1'], digits=2)\n",
    "print(\"Classification Report on holdout data with optimal threshold:\")\n",
    "print(report_optimal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
